# AdaBoost

## 算法描述

集成学习是用于使用多个学习者来解决一个问题的处理方法。通过一个集成的泛化能力通常比单个的学习者要强，因此，集成方法充满了吸引力。Yoav Freund 和Robert Schapire所发表的AdaBoost算法是目前最重要的集成方法之一，因为其拥有坚实的理论基础、非常高的准确率、十分简便（Schapire说他只需要10行代码）和广泛且成功的应用。

令X表示实例空间，y表示类标签的集合。假设y={-1，+1}。给定一个弱的或者基础的学习算法和训练集{(x_1, y_1), (x_2, y_2), … , (x_m, y_m)},这里 xi ∈ X 并且 yi ∈ Y (i = 1, … ,m)，AdaBoost算法的过程如下：

· 首先，它给每个训练实例(xi , yi )(i ∈ {1, … ,m})分配相同的权重。把第t次的学习回合的权重的分布表示为D_t。通过调用基础的学习算法，这个算法从训练集合和D_t中生成一个弱的或者基础的学习者h_t：X->Y。
· 然后，使用训练实例来检测h_t，错误分类实例的权重会增加。这样就可以得到一个更新的权重分布D_t+1。通过再次调用基础的学习算法，AdaBoost从训练集合和D_t+1中生成另一个弱学习者。

这样的过程被重复T轮后，最后的模型就可以由T个弱学习者进行加权多数投票导出，其中学习者的权重在训练的过程决定。在实际的情况中，基础学习算法可能是一个能直接使用加权训练实例的学习算法；否则权重能够通过根据权重分布D_t对训练实例进行抽样来利用。AdaBoost的伪代码如下图所示：

!()[https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/
7.1.png]

为了处理多个类的问题，Freund和Schapire提出了AdaBoost.M1算法,它要求弱学习者即使在Adab过程中产生的硬分布上也要足够强大。AdaBoost另一个流行的多类版本是Adab.MH，它通过将多类任务分解为一系列的二元任务来工作。也研究过用于处理回归问题的AdaBoost算法。由于AdaBoost的各种变种在过去的十年间都得到了发展，Boosting已经成为了集成方法中最重要的一个“家族”了。

## 算法的影响

正如在之前所提到的，AdaBoost是最重要的集成方法之一，所以在各处见到它的重要影响就并不奇怪了。在这部分，我们只简短的介绍两个问题，一个是理论上的，一个是其他的应用。

1998年，Kearn和valiant提出了一个有趣的问题，例如，弱学习算法（学习的准确率也就比瞎猜好上一点）能否被“增强”为一个任意精确的强学习算法。换句话说，两个复杂的类，弱可学习和强可学习问题是否是相同的。Schapire发现这个问题的答案是“yes”，他给出的证明是建设性的，这也就是最早的Boosting算法。因此，很明显AdaBoost的诞生是具有理论意义的。AdaBoost也促进了集成方法理论方面的大量研究，这些研究在机器学习领域和统计领域都可以轻易找到。值得一提的是Schapire和Freund的AdaBoost论文获得了全球奖，这也是2003年在计算机理论科学中最著名的奖之一。

AdaBoost和它的变种应用在很多领域，并且取得了巨大的成功。举例而言，Viola和Jones将AdaBoost和一个级联过程组合起来用于人脸识别。他们在人脸识别中将三角特征看作弱学习者，然后通过使用AdaBoost来为弱学习者进行加权得到了直观的特征。为了获得高的准确率和效率，他们使用了一个级联的过程（这超过了本文涉及的范畴）。结果就是他们建立了一个强大的脸部识别仪：在一个466MHz的机器上，在384*288的图片上的脸部识别只花费了0.067秒，这比当时最先进的人脸识别机器快15倍，同时还有可比的准确性。这个人脸识别被视为过去的十年间计算机视觉（尤其是人脸识别）中最重大的突破之一。这就难怪AdaBoost成为了计算机视觉和许多其他应用领域中的流行词了。

## 未来的研究

有很多有趣的地方值得进一步的研究。这里我们只讨论一个理论上的点和一个应用的点。

很多经验研究表明Adaboost通常不会过拟合，举例而言，即使在训练错误为零的情况下，AdaBoost的测试错误也常常呈现下降的趋势。很多研究者对此进行了研究，同时提出了几个理论上的解释，例如Scahpire等人提出了一个基于边缘的解释。他们认为AdaBoost即使在训练误差已经是零的情况下也能够提高边界，这样即使在经过多轮循环之后，也不会过拟合。然而，Breiman表示更大的边缘并不一定代表着更好的泛化，这对基于边缘的解释提出了巨大的挑战。最近，Beyzin和Schapire发现Breiman考虑的是最小的边缘而不是平均的或者是中间的边缘，因此基于边缘的解释依旧有机会成立。如果这个解释成功了，就能找到AdaBoost和SVM之间的强联系了。很明显这个话题很值得研究。

很多现实中的应用都伴随着高的维度，例如，有很多的输入特征。有两种方式可以帮助我们处理这种类型的数据，例如降维和特征提取。降维的方法通常基于数学投影，它尝试把原来的特征转换到一个适当的特征空间中。在维数减少后，特征原来的含义通常就没有了。特征提取方法是直接提取一些原始的特征来加以利用，因此可以保留特征原来的含义，这在很多应用中很有必要。然而，特征选取方法通常基于启发式规则，它缺少坚实的理论基础。受到Viola和Jones工作的启发，我们认为AdaBoost在未来的特征提取中很有用，尤其是考虑到它有坚实的理论基础。目前的研究主要集中于图像，然而我们认为一般基于AdaBoost的特征提取技术同样值得研究。
