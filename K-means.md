# K-means 算法

## 算法

K-means算法是一个简单的迭代算法，它能够将给定的数据集分割为k（用户定义的数量）个集群。有很多研究者在不同的研究学科中都使用了这个算法，其中最著名的属Lloyd(1957,1982),Forgey(1965),Friedmanand Rubin(1967)和McQueen(1967)。k-means的详细历史及其几个变体的详细描述见论文(Jain AK,Dubes RC(1988)Algorithms for clustering data. Prentice-Hall, Englewood Cliffs)。Gray和Neuboff将k-means置于登山算法文章中，并为其提供了一个良好的历史背景。

这个算法作用在d维的向量上，D={x_i|i=1,…,N},其中![](Eqn1.gif)代表第i个数据点。通过选择![](Eqn2.gif)中的k个点作为初始k个集群的代表或者叫做“重心”来初始化该算法。选取这些初始种子的技巧包括随机从数据集里抽取样本点，并把他们作为从数据中聚类出一个小子集或者扰动数据全局均值k次的解。然后这个算法在下面两步之间进行迭代直到收敛：

- 1 *分配数据* 每个数据点被分配给它最近的重心，任意切割。最终将数据分割开来。
- 2 *重定位“平均值”* 重新定位每个簇代表到包含它的数据集合的中心（平均）。如果数据点带有概率测度（权重），那么把这个点重新定位为数据集的期望（加权平均）。

当代表聚类的点（并且因此值![](Eqn3.gif)）不再改变时算法收敛。图1描述了算法执行的可视化过程。注意每次迭代都需要进行N*k次比较，这决定了一次迭代的时间复杂度。为了收敛需要迭代的次数是变化的，有可能取决于N，但是作为第一次切割，算法可以视为和数据集的大小呈线性关系。

一个需要解决的问题就是如何在分派点的时候对“最近”进行定义。“最近”的默认测量方式是使用欧几里得距离，在这种情况下可以便利地使用下面的非负成本函数![](1.png),在每次分配或重定位之后，这个值都会变小，因此保证了经过有限的迭代之后一定会达到收敛。非凸成本下k-means的贪婪下降性质也暗示着收敛只是局部最优，并且事实上算法对初始重点的选取很敏感。图2展示了和图1中数据相同时，选取了另外三个初始重心得到了更差的结果。局部最小值问题在某种程度上可以通过用不同的初始重心多次运行算法，或者通过对融合解做有限的局部搜索来解决。

![](2.png)

![](3.png)

## 局限性

k-means算法除了对初始情况比较敏感之外，算法还有几个其他的问题。首先，当应软分配到混合部分上的数据点被强制分配到他们各自最有可能的部分上时，k-means适用于拟合数据的有限情况，它通过对一致、各向同性的协方差矩阵（![](Eqn4.gif)）进行k阶高斯混合。所以，如果数据没有很好的被独立的球面所包裹的时候，就会出现模棱两可的状态，例如在数据中有非凸形状的簇时。通过在聚类之前重调数据来“漂白”它可以缓解这个问题，除此之外也可以使用不同的更适合数据集的计算数据点间距的方法。例如，信息理论集群使用KL-离散量来衡量两个数据点之间的距离来代表两个离散概率分布。最近的研究表明如果在分配步骤中选择称为布雷格曼离散量的大类中的一个来衡量距离，并且不进行其他改变，那么k-means的重要属性包括保证收敛性、线性分离边界和可扩展性都可以被保留。这个结果使得只要使用了适当的离散量，k-means方法对于拥有更多数据集合的类也有效。

k-means可以结合其他算法来描述非凸簇群。首先使用k-means把数据聚类成许多的组。然后使用能够检测复杂形状的单链路层次聚类把这些组聚合成更大的簇群。这个方法也能让结果降低对初始点的敏感度，并且由于分层方法能够提供多个结果，所以不需要预先指定k。

随着k的增长最佳解的消耗逐渐减少直到变成零，此时集群的数量和不同数据点的数量相同。这使得直接比较有不同数量的集群的解和找到最佳的k值变得更困难。如果需要的k事先是不知道的，那么一般首先会用不同的k值来运行k-means，然后使用一个合适的标准来选取其中的一个结果。例如，SAS使用了多维数据集的聚类准则（cube-clustering-criterion），而X-means在原始损耗函数中增加了复杂标度（随着k增长）然后把k定义为能够最小化损耗的值。另外，可以逐渐增加集群的数目直到一个合适的停止标准。平分k-means方法实现了这个想法。首先把所有数据放到同一个聚类中，然后不断的使用2-means方法把最不紧密的集群进行切分。著名的用于矢量量化的LBG算法让簇的数量翻倍直到获得一个合适的码书大小。所有的这些方法都减轻了对于提前知道k值的要求。

这个算法对于离散的值也十分敏感，因为“均值”不是稳健统计的。在预处理中去除离散的点会很有用。得到结果后的处理，例如去除小的聚类，或者把一些相近的聚类合并为一个更大的类也是不错的。Ball和Hall在1967年的ISODARA算法在k-means算法中有效的使用了预处理和后处理方式。

## 泛化和连接

正如之前所提到的，k-means与将混合的k个各向同性高斯与数据拟合有关。此外，对于所有布雷格曼离散量的距离测量的泛化与将数据和k个来源于指数族分布的元素的混合进行拟合有关。另一个粗略的泛化是将“均值”看作概率模型而不是![](Eqn2.gif)中的点。因此，在分配步骤中，每个数据点被分配给最有可能产生它的模型。在重定位步骤中，模型的参数被更新到最好以适应分配的数据集。这种基于模型的k-means方法允许使用者使用一些更复杂的数据，例如在隐马尔科夫模型中描述的序列。

使用者也可以“核化”k-means方法。尽管集群之间的边界在高维空间中仍是线性的，但当他们投影到原来的空间时他们就可以变成非线性的，因此使用核k-means可以处理复杂的集群。Dhillon等人已经展示了核k-means和谱聚类之间密切的关系。K-medoid算法十分类似于k-means算法，除了它的重心必须是聚类数据集中的一个点。模糊c-means也是一个类似的例子，除了她为每个集群而不是艰难的那个计算模糊隶属度函数。

尽管有这些缺点，k-means也是在实际中最广泛使用的分隔式聚类算法。它的算法简单，容易理解，可以进行合理的扩展，并且能够进行简单的修改以适应流数据。对于非常大的数据集，进一步提高k-means速度的工作已经进行实质性的努力了，最著名的是使用kd-树或者利用三角不等式来避免在分配步骤中把每个数据点和所有的重心进行比较。基础算法的持续改进和泛化确保了它的持续相关性并且也在逐渐增长它的有效性。
