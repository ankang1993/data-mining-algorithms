在今天的机器学习应用之中，支持向量机（svm）是一定值得尝试的————在所有著名的算法中，它提供了最健壮和准确的方法。它有著名的理论基础，只需要为数不多的训练样本并且对于维数不敏感。除此之外，训练SVM的有效方法正在飞速发展。 

在一个二类学习问题中，SVM的目标是找到能够区分训练数据中的两个类的最好的分类函数。衡量“最好”的分类函数的标准可以在几何上实现。对于一个线性可分的数据集合，一个线性分类器函数对应于穿过两个类中间，把两个类分开的超平面f（x）。一旦这个函数被确定了，新的数据实例x_n能够通过测试函数f（x_n）的符号来进行分类；如果f（x_n）>0那么x_n就属于正类。 

因为有很多这样的线性超平面，SVM额外保证了这个最好的函数是通过最大化两个类之间的距离来得到的。直观地讲，这个间距被定义为空间的量，或者超平面所定义的两个类之间的间隔。几何上来看，这个间距对应于超平面上的一个点和跟它距离最近的数据点之间的最短距离。有了这样的几何学的定义之后，我们可以探寻如何最大化这个间距，从而使得即使有无限多的超平面，但也只有几个可以作为SVM的解。

SVM之所以要找到间距最大的超平面是因为这样的超平面提供了最好的泛化能力。这使得这个超平面不仅对于训练数据有最好的分类效果（例如准确度），而且预留了足够的空间给将来需要正确分类的数据。为了确保最大间距的超平面能够真正被找到，一个SVM分类器会尝试分别对w和b最大化下面的函数：

![](3.1.png)

这里的t是训练样本的数量，α_i(i=1…t)是非负的数，这样可以保证L_p的倒数关于α_i为0。α_i是拉格朗日乘子，L_p被称为拉格朗日算符（Lagrangian）。在这个方程中，向量w和常数b定义了超平面。

关于上面支持向量机的基本公式这有几个重要的问题和相关的扩展。我们在下面列举了这些问题和扩展：

1. 我们可以通过坚实的理论基础理解SVM的含义吗？ 
2. 我们可以扩展SVM的公式来处理一些我们允许错误存在的情况？即使最好的超平面都必须接受训练数据中的错误。
3. 我们能扩展SVM的公式使它在训练数据非线性可分的情况下也能适用吗？
4. 我们能够扩展SVM使得它不仅能够处理分类问题，也可以预测数值或者列出将这些实例当作正类的可能性？ 
5. 我们能缩放算法的规模使得它可以在数以千计的实例中找到最大间距的超平面吗？

### 问题1 我们可以通过坚实的理论基础理解SVM的含义吗？

在这个问题中存在着几个重要的理论结果。 

诸如SVM这样的学习机可以被定义为基于参数α的函数集。不同的函数集合有不同的学习能力，这个学习能力可以用VC维的参数h表示。VC维衡量了不管给数据点分配什么类标签，函数都能以0的错误率完美的区分所有的训练数据所需要的最大的训练实例数。可以证明未来数据的实际错误由两个方面的总和界定。第一个方面是训练错误，第二个方面是与VC维h的开方成比例。因此如果我们可以减小h，那么我们就可以减少未来的错误。事实上，通过SVM得到的最大化间距的函数就是这样的一个函数。因此，SVM有坚实的理论基础。

### 问题2 我们可以扩展SVM的公式来处理一些我们允许错误存在的情况？即使最好的超平面都必须接受训练数据中的错误。

为了回答这个问题，首先想象有几个不同类的点跨越了中间。这些点代表了即使是在最大间隔的超平面里还是存在训练错误。“软间距”的思想扩展了SVM的算法使得它能够容许在分类的时候存在一些噪声数据。特别的引入了松弛变量ξ_i来负责评估违反分类函数f（x_i）的点的数量。ξ_i有几何学上的解释：从分类错误的数据实例到超平面的距离。这样，引入的松弛变量总的损失函数就能够用来修正原始的目标最小化函数。

### 问题3 我们能扩展SVM的公式使它在训练数据非线性可分的情况下也能适用吗？ 

通过观察目标函数，我们可以得到这个问题的答案。在目标函数中，唯一使用到向量x_i的情况就是点乘。也就说，如果我们使用函数映射Φ（x_i）把x_i映射到另一个更大甚至可能是一个无限维的空间H中，从而扩展点乘x_i•x_j,然后保持方程不变。在每个方程中都有点乘x_i•x_j，我们现在把它转化为了被称为核函数的Φ（x_i）•Φ（x_j）。

核函数能够用于定义输入之间的多样的非线性关系。举例而言，除了线性核函数之外，你可以定义二次或者指数型的核函数。近年来的很多研究就是关于SVM分类中不同的核函数和很多其他的统计测试。我们也可以把上面描述的二分类的SVM扩展为多分类的分类器。通过重负的将一个类作为正类，其他类作为负类就可以达到这个目的（这个方法就是著名的 one-against-all 方法）。

### 问题4 我们能够扩展SVM使得它不仅能够处理分类问题，也可以预测数值或者列出将这些实例当作正类的可能性？

SVM可以很容易的扩展用来数值计算。这里我们讨论两个不同的扩展。第一个是扩展SVM来进行回归分析，回归分析的目的是产生一个接近于目标函数的线性函数。需要仔细的考虑错误模型的选择；在支持向量回归机（SVR）中，如果预测的值和实际的值在ε范围内那么错误就定义为0。否则，ε不敏感的错误就会线性增长。支持向量机可以通过最小化拉格朗日算符来求取。支持向量回归机的一个优点就是对于离散的点不敏感。

另一个扩展就是给元素排序而不是产生一个独立的元素的分类。排序可以简化为比较实例对，如果这一对排序是正确的，那么就得到a+1的评价，否则就得到-1的评价。因此简化这个任务为SVM学习的一种方式就是为训练数据里每个排序的实例对构造新的实例，然后对新的训练数据使用超平面进行学习。

这个方法可以应用到很多排序是很重要的领域，比如信息检索领域中的文档排序。

### 问题5 我们能缩放算法的规模使得它可以在数以千计的实例中找到最大间距的超平面吗？

　　SVM的最开始的缺点之一就是计算性能比较差。然而这个问题的解决已经取得了巨大的成功。其中的一个方法就是把一个大的优化问题转化为一系列更小的问题，这些小的问题里面只包含了少数谨慎提取出来的变量，因此可以有效的解决优化问题。这个过程一直迭代到所有的分解的优化问题都得到了成功的解决。一个更前沿的方法就是把SVM学习的问题看成寻找实例集中的一个近似的最小包围球。

这些实例在映射到N维空间的时候，代表了一个能够用于构建近似最小包围球的核心集合。通过这些核心集合来解决SVM的学习问题能够快速的得到一个很好的近似结果。举例而言，这样构造的核心向量机（core-vector machine）能够在一秒内学习一个有数百万数据的SVM。
