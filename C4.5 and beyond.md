# C4.5 and beyond

## 介绍

构造分类器系统是在数据挖掘领域中普遍使用的工具之一。该系统将一系列的事件作为输入，每个事件都属于其中的一个类,并且通过他们固有的一系列属性的值对其进行描述。最后产生一个能够准确预测新的case属于哪个类别的分类器。

C4.5源于CLS和ID3。和CLS和ID3一样，C4.5使用决策树表示来生成分类器，但它也能够以更加容易理解的规则集形式来构造分类器。论文中会大概描述C4.5采用的算法，并且介绍其“后继者”See5/C5.0的一些改变，并且利用一些公开的研究进行总结。

## 决策树

给定一系列的事件集S，C4.5首先利用分治算法构建初始的树，分治算法如下：

- 如果S中的事件都属于同一个类，或者事件集S很小，那么该树就变成一个叶子，使用S中频率最高的类定义这个叶子。

- 否则，选择一个测试，而这个测试在具于相同属性的事件上有两个或者更多的结果。让这个测试成为树的根，并且让每个结果成为该测试的一个分支。根据每个事件的结果将S分为子集S1、S2···，并且对他们的子集递归的使用这个方法。

通常在最后一布有许多测试可以选择。C4.5使用两个启发式的方法对这些测试进行排序：信息增益，它使得子集{Si}的熵最小（但是会偏向于数值输出多的测试），并且默认的增益率由测试输出来提供，增益率等于信息增益除以信息。

属性可以是数值型的也可以是名义上的，这也决定了测试结果的格式。对于数值型的属性A，测试结果为{A<=h,A>h},这里的临界值h是通过将S按照数值A进行排序，然后在连续的数值中进行划分从而使得上述标准最大化而选取的。属性A的不同离散值默认对应一个结果，但是允许将数值组合进两个或者多个有各自结果的的子类。

接下来会通过修剪防止初始树过度拟合。修剪算法基于一个包含N个例子集合错误率的消极估计，其中有E个不属于频率最高的类。C4.5规定了当在N个实验中观察到E个事件时二项式概率的上限，使用默认值为0.25的用户指定置信度来取代E/N。

修剪是从叶子到根部进行的。叶子上对于N个事件，E次错误的估计错误为N乘以上述的消极错误率。对于子树，C4.5增加分支的估计错误，并且把这个估计错误与使用叶子替换分支后的估计错误相比较，如果前者比后者高，子树就要被修剪。类似的，C4.5检查子树被其分支替代的估计错误，当替换有利时，也要对树进行相应的修改。修剪的过程是在对树进行一次遍历下完成的。

C4.5的树构造算法与CART的算法在以下几个方面不同： 
- CART中的测试是二元的，但是C4.5允许有两个或者多个结果
- CART使用基尼多样性指数来排序测试，而C4.5使用基于信息的准则
- CART使用参数由交叉验证评估的成本复杂度模型来修剪树；C4.5使用基于二项置信度的单一路径算法 
- 这个简短的讨论中没有涉及到当在一些数据值不清楚的情况下应该怎么办。当测试属性有未知数值时，CART会结果相似的替代测试，而C4.5则将事件概率的分摊到结果之中。

## 规则集分类

复杂的决策树理解起来是十分困难的，因为有可能关于一个类的信息是分布在树的各个地方的。C4.5引入了一个包含一系列规则的替代形式，规则形式为“ifA和B和C则为类X”，每个类的规则组合在一起。一个事件根据第一个条件满足的规则进行分类。如果没有规则能够对其进行分类，那么便将它分给默认的类。

C4.5的规则集是由未修剪的初始决策树生成的。每条从树的根到叶子的路径都会变成一个初始规则，它的条件是路径的结果，类型为叶子节点的标签。然后这个规则会轮流通过判定舍弃每个条件的影响来进行简化。舍弃一些条件可能会增加满足规则的事件数N和不属于该规则管理的类的事件数E，因此会降低上述得到的消极错误率。登山算法是用来减少条件直到达到最低的消极错误率。

为了完成这个过程，简化规则的子集会轮流被每个类选中。将这些类子集进行排序，从而最小化训练集的错误，然后选出默认的类。最终的规则集的数量通常比决策树叶子节点的数量更少。

C4.5规则集的最主要的缺点是他们消耗的CPU运行时间和内存容量。在一个试验中，从一个大数据集中提取的范围从10,000到100,000的样本。对于决策树而言，从10到100k的事件消耗的cpu时间从1.4增长到64s，增长率为44。然而规则集所需的时间从32增长到了9715s，增长率为300。

## See5/C5.0

