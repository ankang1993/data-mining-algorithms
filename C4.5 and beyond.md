# C4.5 and beyond

## 介绍

构造分类器系统是在数据挖掘领域中普遍使用的工具之一。该系统将一系列的事件作为输入，每个事件都属于其中的一个类,并且通过他们固有的一系列属性的值对其进行描述。最后产生一个能够准确预测新的case属于哪个类别的分类器。

C4.5源于CLS和ID3。和CLS和ID3一样，C4.5使用决策树表示来生成分类器，但它也能够以更加容易理解的规则集形式来构造分类器。论文中会大概描述C4.5采用的算法，并且介绍其“后继者”See5/C5.0的一些改变，并且利用一些公开的研究进行总结。

## 决策树

给定一系列的事件集S，C4.5首先利用分治算法构建初始的树，分治算法如下：

- 如果S中的事件都属于同一个类，或者事件集S很小，那么该树就变成一个叶子，使用S中频率最高的类定义这个叶子。

- 否则，选择一个测试，而这个测试在具于相同属性的事件上有两个或者更多的结果。让这个测试成为树的根，并且让每个结果成为该测试的一个分支。根据每个事件的结果将S分为子集S1、S2···，并且对他们的子集递归的使用这个方法。

通常在最后一布有许多测试可以选择。C4.5使用两个启发式的方法对这些测试进行排序：信息增益，它使得子集{Si}的熵最小（但是会偏向于数值输出多的测试），并且默认的增益率由测试输出来提供，增益率等于信息增益除以信息。

属性可以是数值型的也可以是名义上的，这也决定了测试结果的格式。对于数值型的属性A，测试结果为{A<=h,A>h},这里的临界值h是通过将S按照数值A进行排序，然后在连续的数值中进行划分从而使得上述标准最大化而选取的。属性A的不同离散值默认对应一个结果，但是允许将数值组合进两个或者多个有各自结果的的子类。

接下来会通过修剪防止初始树过度拟合。修剪算法基于一个包含N个例子集合误差率的消极估计，其中有E个不属于频率最高的类。C4.5规定了当在N个实验中观察到E个事件时二项式概率的上限，使用默认值为0.25的用户指定置信度来取代E/N。

修剪是从叶子到根部进行的。叶子上对于N个事件，E次错误的估计错误为N乘以上述的消极误差率。对于子树，C4.5增加分支的估计错误，并且把这个估计错误与使用叶子替换分支后的估计错误相比较，如果前者比后者高，子树就要被修剪。类似的，C4.5检查子树被其分支替代的估计错误，当替换有利时，也要对树进行相应的修改。修剪的过程是在对树进行一次遍历下完成的。

C4.5的树构造算法与CART的算法在以下几个方面不同： 
- CART中的测试是二元的，但是C4.5允许有两个或者多个结果
- CART使用基尼多样性指数来排序测试，而C4.5使用基于信息的准则
- CART使用参数由交叉验证评估的成本复杂度模型来修剪树；C4.5使用基于二项置信度的单一路径算法 
- 这个简短的讨论中没有涉及到当在一些数据值不清楚的情况下应该怎么办。当测试属性有未知数值时，CART会结果相似的替代测试，而C4.5则将事件概率的分摊到结果之中。

## 规则集分类

复杂的决策树理解起来是十分困难的，因为有可能关于一个类的信息是分布在树的各个地方的。C4.5引入了一个包含一系列规则的替代形式，规则形式为“ifA和B和C则为类X”，每个类的规则组合在一起。一个事件根据第一个条件满足的规则进行分类。如果没有规则能够对其进行分类，那么便将它分给默认的类。

C4.5的规则集是由未修剪的初始决策树生成的。每条从树的根到叶子的路径都会变成一个初始规则，它的条件是路径的结果，类型为叶子节点的标签。然后这个规则会轮流通过判定舍弃每个条件的影响来进行简化。舍弃一些条件可能会增加满足规则的事件数N和不属于该规则管理的类的事件数E，因此会降低上述得到的消极误差率。登山算法是用来减少条件直到达到最低的消极误差率。

为了完成这个过程，简化规则的子集会轮流被每个类选中。将这些类子集进行排序，从而最小化训练集的错误，然后选出默认的类。最终的规则集的数量通常比决策树叶子节点的数量更少。

C4.5规则集的最主要的缺点是他们消耗的CPU运行时间和内存容量。在一个试验中，从一个大数据集中提取的范围从10,000到100,000的样本。对于决策树而言，从10到100k的事件消耗的cpu时间从1.4增长到64s，增长率为44。然而规则集所需的时间从32增长到了9715s，增长率为300。

## See5/C5.0

C4.5在1997年被商业系统See5.0/C5.0(简称为C5.0)取代。变化带来了一些新功能的同时也大幅提高了效率，包括：

- 推进（boosting）的一种变体，它构建了一个用于决定最终分类的分类器的集合。推进常常会戏剧性的提高预测准确度。
- 新的数据类型（例如日期），“不适用的”数值，变量误分类的代价和提前过滤属性的机制。
- 无序的规则集---当一个事件被分类时，所有可用的规则都被找到并用来投票。这既可以提高规则集的解释性，也能提高它们的预测准确性。
- 极大的提高了决策树和（特别是）规则集的可扩展性。可扩展性的提高是通过多线程的方式得到的；C5.0能够利用多个CPU和/或多个核心。

[更多的详细信息](http://rulequest.com/see5-comparison.html)

## 研究问题

我们经常听到同事说决策树已经是一个“解决了的问题”。我们对此并不同意，并且我们会讨论几个开放性的研究问题。

*稳定树* 众所周知，构建过的决策树的误差率（重构误差率）常常小于看不见它的情况（预测误差率）。举例而言，一个很有名的字母识别的数据集有20，000个事件，C4.5的重构误差率为4%，然而它的留一法（20,000倍）交叉验证的误差率却为11.7%。如上所示，不考虑20，000个事件中的某一个事件对构建过的树有很大的影响。

假设现在我们可以研究出几乎不受忽略单事件影响的超凡的树构造算法。对于这样稳定的树来说，重构误差率应该和除去其中一个事件的交叉验证率非常接近，这表明该树处于“正确的”规模。

*分解复杂树* 无论是通过推进（boosting），套袋（bagging），权重正则化或者其他的技术，集成分类器通常都会表现出更高的准确性。现在，给定少量的决策树完全可能产生一个单独的（复杂）树，它完全等同于投票初始树，但我们可以利用另一种方式吗？也就是说一个复杂的树能够分解成为一些简单树的集合吗？并且在一起投票时能够给出和复杂树相同的结果。这样的分解会极大的帮助产生容易理解的决策树。

## C4.5致谢

澳大利亚研究委员会资助C4.5的研究多年。

C4.5可以免费用于研究和教学，它的源码可以从[这里下载](http://rulequest.com/Personal/c4.5r8.tar.gz)
