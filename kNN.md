# KNN：k-nearest neighbor classification

## 算法描述

机械分级器是目前最简单也是最琐碎的分类器之一，它记录了所有的训练数据，而且仅当测试对象的属性完美吻合训练对象其中之一时才能完成分类。这个方法的一个明显的缺点就是很多测试记录无法进行分类，因为他们不能精确的匹配到训练记录中的任何一个。一个更加高明的方法，k邻近分类法（KNN），是从训练集中找到k个与测试对象最接近的一组对象，然后把这个群中的具有优势的那一类作为这一组的标签。这个方法有三个关键元素：有标签的对象集，例如，一组储存记录的集合；用来计算对象之间距离的差距或相似的标准；最接近邻居的数量k的值。为了对没有标签的对象进行分类，就需要计算这个对象和有标签对象之间的距离，它的k个最近的邻居就会被识别出来，并且这些最近的邻居的类标签在之后就会被当作这个对象的类标签。

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/8.1.png)

上图展示了邻近分类法的简要总结。给定一个训练集D和测试对象x=(x',y'),算法计算出z和所有训练对象（x,y）∈D的距离（或者相似度）来得到它的最近邻居列表D_z。(x是训练对象数据，y是它的类别。类似的，x'是测试对象数据，y'是它的类别)

一旦得到了最近邻居列表，测试对象的类别就可以根据最近邻居中大多数的类别来确定：

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/8.2.png)

这里的v是类标签，y_i是第i个最近邻居的类标签，并且I（·）是一个指示函数，如果里面的内容为真它返回1，否则就返回0。

## 问题

有几个影响KNN性能的关键问题。一个是k值的选择。如果k值太小，结果对噪声点就会很敏感。另一方面，如果k值太大，邻居就可能会包含太多其他类别里的点。

另一个问题就是组合类别标签的方法。最简单的方法就是多数投票，但是如果最近邻居跨越了太多区域，并且较近的邻居对于指示对象的类别更加可靠的话，这就会出问题。一个比较机智并且对k值的选择不是那么敏感的方法就是根据每个对象的距离来对他们的投票进行加权。这里的权值通常设为距离平方的倒数：wi=1/d(x',x_i)^2。这相当于将KNN算法的最后一步替换为如下的表示：

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/8.3.png)

距离测度的选择是另一个值得考虑的问题。虽然有很多方法可以用于计算两点之间的距离，但最好的距离计算方法应该是在如果两个对象之间的距离更小，那么暗示着他们有更大的可能性是具有相同的类。因此举例而言，如果kNN被用于文档分类，那么使用余弦测度的效果就会比使用欧几里得距离更好。一些距离测量方法也会受到数据高维的影响。尤其是众所周知，欧几里得距离随着属性数量的增长，效果会变的越来越差。同样的，属性或许可以经过标准化，从而避免一个属性对距离测度的影响巨大。例如，假如有一个数据集里人的身高在1.5m-1.8m之间变化，体重的范围为90到300lb，以及收入水平为从$10,000到$1,000,000。如果没有标准化距离测度，收入属性就会主宰距离计算的结果，从而影响类标签的分配。有一些被提出来的方法试图基于训练集计算每个不同属性的权重。

除此之外，训练对象本身也可以被赋予权重。我们可以给高度可靠的数据更高的权重，同时给降低不值得信赖数据的权重。由Cost和Salzbery提出的PEBLS系统就是一个这样的著名例子。

KNN分类是一个懒学习者，也就是说，模型不是像饥饿学习者（例如决策树等）那样被精确的进行构建。因此，虽然构建模型是很方便的，但是在对不明确的对象进行分类时代价也是高昂的，因为它要计算对象的k个最近的邻居来为它打标签。通常需要计算没有标签的对象和有标签集合里所有对象的距离，这对大型的训练集合尤其昂贵。有许多方法被用来有效的计算k个最近邻居的距离，它们通过使用数据的结构来避免对数据集里所有的对象计算距离。这些技术尤其适用于低维度的数据，在大大降低计算的复杂程度的同时，还能保留分类的准确性。

## 影响

KNN分类不仅容易理解而且容易实现分类技术。除了简单之外，它也适用于许多情境。其中特别著名的是在某种特定的合理假设下，由Cover和Hart所展示的最邻近邻居法则的错误小于贝叶斯错误的两倍。同样，一般kNN方法的错误渐渐的接近贝叶斯的错误，因此可以用来估计它。

KNN特别适合多模式分类和一个有多种类别标签的对象。举例而言，在利用表达谱来判断基因的功能的时候，一些研究者发现kNN的效果优于SVM，而SVM是一种更复杂的分类方法。

## 目前和未来的研究

虽然基本的kNN算法及例如有权重的kNN算法和对象有权重的kNN算法之类的变种相对比较有名，但是kNN的一些更先进的技术却不是那么有名。例如，有可能在剔除了许多存储的对象之后，kNN分类器的准确性还是那么好。这被称为“压缩”，这种方法可以大幅提高新对象的分类效率。除此之外，可以移除数据对象来提升分类的准确率，这个过程被称为“编辑”。在邻近图（最近邻居图，最小旋转树，相对领域图，三角网和Gabriel图）上使用kNN还有许多可观的工作可以做。最近在Toussaint的论文中强调了邻近图的观点，它提供了一个解决这三个方面的工作的综述，并指出了一些仍然开放的问题。其他重要的资源包括Dasarathy的采集论文和Devroye等人的书。最后，在Bezdek的书中可以找到kNN的一个模糊方法。
