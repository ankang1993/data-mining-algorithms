# Naive Bayes

## 引言

给定一个对象的集合，已知他们的变量向量和所属类别，我们的目标是创建一个规则，给定一个新的对象的输入向量，通过这个规则，我们能够预测它的类别。这样的问题很普及，叫做监督式分类问题，目前有很多用于监督式分类的方法被创造和发展。其中有一个很重要的就是朴素贝叶斯方法，同时也被称为idiot's Bayes，simple Bayes和independence Bayes。这个方法之所以很重要，主要是因为以下几个原因：构建简单，不需要任何复杂的迭代参数评估过程。这意味着该方法适用于巨大的数据集。容易解释，对分类技术不熟悉的人也能理解为什么这个模型是有用的。最后，它的表现也常常不错。它或许不是某一个特定应用中的最好的分类器，但是它通常是健壮的，并且对分类的效果不错。在另一篇论文中给出了朴素贝叶斯方法的概述和它的优点。

## 基本概念

为了便于展示，我们假设所分的类别只有两类，即i=0，1。我们的目标是通过使用已知类关系（训练数据）的初始对象集合来构造一个评分，让大的评分与类1对象(假定)相关，同时小的评分与类0对象相关。这样分类问题就可以通过比较评分和阈值t的大小来解决了。如果我们定义p(i|x)为以x=(x_1,..,x_p)为特征向量的对象属于类别i的概率，这样任何P(i|x)的单调函数都可以构造一个合适的评分。特别低，P(1|x)/P(0|x)就是个很好的例子。初等概率告诉我们可以把P(i|x)分解为f(x|i)P(i),这里的f(x|i)是向量x对于类别i对象的条件分布，p(i)是对象属于i的概率，如果我们对它什么都不知道（那概率就是先验概率）。于是可以有下面的表示：

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/9.1.png)

为了使用这个分类器，我们需要估计f(x|i)和P(i)。如果训练集是总样本的随机分布，那么P(i)就可以直接由训练集中类别i的比例来决定。为了估计f(x|i)，朴素贝叶斯方法假定x的成分都是独立的，于是f(x|i)=![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/9.2.png)，这样我们就可以分别估计单变量分布
f(x_j|i),j=1,…,P;i=0,1中的每一个了。这样p维的多变量问题就可以简化为p个单变量问题。单变量问题很熟悉，也很简单，并且为了得到准确的估计所需要的训练集更小。这也是朴素贝叶斯方法独特的真正独一无二的吸引力所在：估计简单，十分快速，不需要复杂的迭代评估过程。

如果边缘分布f(x_j|i)是离散的，每个x_j只有几个值，那么估值f(x_j|i)就是一个“多项直方类型估计量”(multinomial histogram type estimator)——简单的计算落入不同区域的类别i对象的比例。如果f(x_j|i)是连续的，一种常见的做法是将他们他们分为几个不同的区间，然后再次使用多项式估计量，但是基于连续估计(例如核估计)的比较复杂的方式也同样是有人使用的。

假设相互独立，上式就变成了

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/9.3.png)

现在，由于我们的目标仅仅是计算一个与P(i|x)单调相关的评分，我们可以对上式加对数——对数是一个单调递增函数。于是我们就可以得到下面的替代分数

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/9.4.png)

如果我们定义w_j=ln(f(x_j|1)/f(x_j|0)),常量k=ln(P(1)/P(0))，我们可以将上式简化为

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/9.5.png)

这样分类器具有一个特别简单的结构了。

对于每个类中的中x_j之间独立性的假设意味着朴素贝叶斯模型似乎有比较严格的限制。然而事实上，由于有很多因素都可能会发挥作用，这也意味着假设并不是那么有害处。首先，会先进行一个先验变量选取步骤，在这个步骤之中，相关性较高的变量会被消除，因为他们可能会以相似的方式对类进行分离。也就说剩下的变量会是近似独立的。其次，假设没有相互作用，这也提供了一个暗含的正则化步骤，因此它减少了模型的变量同时也能够得到准确率更高的分类器。第三，在某些变量相关的情况下，最优决策面(optimal decision surface)与有独立假设的情况下是一样的，所以做出这样的假设并不会对效果造成不利影响。第四，当然由贝叶斯模型得到的决策面事实上通常有一个复杂的非线性形状：w_j中的表面是线性的，但是与原变量x_j高度非线性，所以它可以适用于相当复杂的表面。

## 一些拓展

除了上面这些内容，很多作者尝试提高朴素贝叶斯方法的预测准确性，并对其做出了改进。

一个比较早提出的改进是缩小落入每个离散预测变量的每个类别的对象比例的简单多项估计。因此，如果第j个离散预测变量x_j有c_r个类别，同时如果总数为n的对象中有n_jr个落入这个变量的第r个分类中，常用来估计未来一个对象落入这个分类中概率的多项式估计n_jr/n就会被(n_jr+(c_r)^(-1))/(n+1)替代。这样的收缩也有一个直观的贝叶斯解释。它的结果就是估计有了更小的方差。

或许消除独立性假设最直观的方式是在每个类的x的分布模型中引入额外的规定，从而允许有相互作用。有很多人尝试过这个方法，但是我们必须认识到这么做一定会引入复杂度，因此这也牺牲了朴素贝叶斯模型基本的简单和优雅。在任意的类中，x的联合分布如下：

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/9.6.png)

通过简化条件概率可以近似得到这个联合分布。最极端的情况就是对于每个i都有f(x_i|x_1,...,x_i−1)=f(x_i)，这就是朴素贝叶斯方法。然而显然模型在这两种极端情况之间都可以使用。例如，可以使用马尔科夫模型：

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/9.7.png)

这等同于在朴素贝叶斯中使用双向边缘分布而不是单变量边缘分布的子集。

朴素贝叶斯模型的另一个扩展是完全独立于它发展的。那便是逻辑回归模型。在上面我们通过采用朴素贝叶斯独立性假设获得了第二个式子。然而，如果我们构造模型![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/9.8.png)，其中函数g(x)在每个模型中是相同的，那么比例结果的结构完全相同。比例如下：

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/9.9.png)

这里，h_i(x_j)并不一定要是概率密度函数——如果![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/9.10.png)是密度的话就足够了。上式中的模型和朴素贝叶斯模型一样简单，并且使用相同的形式——使用对数而且正如我们在第四个式子中总结的一样——但是因为它不需要在每个类中x_j是独立的假设，这个方法也更加灵活。事实上，这个方法通过可以是任何形式的g(x)函数可以允许任意的独立结构。然而重点在于这种独立性在两个类中是相同的，所以它抵消了第七个公式中的比例。当然，这种逻辑回归模型额外可观的灵活性不是没有代价的。虽然结果模型是和朴素贝叶斯模型的形式完全一样(当然，它们有不同的参数)，但它不能通过单独寻找每个边缘分布来进行估计：它必须使用一个迭代的过程。

## 朴素贝叶斯的总结

由于它的简洁、优雅和健壮性，朴素贝叶斯有很强的吸引力。它是最古老的正式的分类器算法之一，然而即使是在最简单的形式它通常也表现的惊人的有效。它被广泛的使用在例如文字分类和垃圾邮件过滤中。统计学、数据挖掘、机器学习和模式识别社区为了让它更灵活，朴素贝叶斯有了大量的改进版本，但是不得不承认这些方法都或多或少的偏离了朴素贝叶斯方法的简洁性，变得更加复杂。一些改进方法在其他论文中有描述。
