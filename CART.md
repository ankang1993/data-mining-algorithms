# CART

1984年由Leo Breiman,Jerome Friedman,Richard Olshen和Charles Stone 共同发表的著作“CART:Classification and Regression Trees”是人工智能、机器学习、无参数统计和数据挖掘中的一个重大里程碑。这项工作对决策树研究的全面性，引进的技术创新，树状结构数据分析的复杂讨论以及对树木大样本理论的权威性处理十分重要。尽管几乎在所有领域中都出现了CART的引用，尤其是在电器工程、生物、医药研究和金融领域中，然而例如在市场研究或者社会学中，其他树方法更流行。这个章节主要为了介绍CART著作中相关的重要主题，从而鼓励读者回到原始的材料中获取更多细节。

## 综述

CART决策树是一个能够将连续的名义属性作为目标和预测因子进行处理的二元递归分区程序。数据使用原始的形式处理；不需要且不推荐装箱处理。在没有使用停止规则的情况下，树可以增长到最高的大小，然后可以通过成本复杂度修剪来修剪（在本质上一点一点分裂）到根。接下来需要修剪的分裂是对于训练数据总体性能表现贡献最小的分支(同一时间可能会修剪多个分支)。这个程序产生的一个在预测属性任意的顺序保持转变下都不变的树。CART的机制在于产生不是一个而是一系列内嵌的修剪树，并且把这些树作为最优树的候选。“大小适合”或是“可靠”的树是通过对修剪序列中每个树的预测表现进行评估的。CART不提供基于训练数据的选择树的内部性能评估，这样测度就会被认为有问题。相反，树的性能往往是通过独立的测试数据(或者通过交叉验证)来验证的，并且选择树是在基于测试数据的评估后进行的。如果没有测试数据并且无法使用交叉验证，CART就无法知道哪个序列是最好的。这与基于训练数据测度生成首选模型的方法如C4.5有着巨大的不同。

CART机制包括了自动(可选)类平衡、自动处理丢失数据，同时允许代价敏感的学习、动态特征构造和概率树估计。最后的报告包括了一个新型的属性重要性排行。CART的作者同时也有了新的突破，展示了如何使用交叉验证评估修剪过程中的每个树的性能，此时假定在不同CV层的树与最后终端节点的个数无关。这些每个主要特征在下面都会被讨论到。

## 分割原则

CART分割原则常常以下面的形式出现：一个实例如果满足条件则在左边，否则在右边。对于连续属性而言这里的条件可以表示为“属性X_i<=C”。对于名词属性而言这的条件可以表示为数值列表的成员。CART作者表明二分割是一种比较好的方法，因为(1)相比较多路分割，它们分割数据更慢(2)可以对同一个属性进行重复分割，如果需要的话，对于一个属性要分割多少片都可以。树可读性的降低可以通过提高树的性能进行弥补。(3)作者开发的大样本定律被限制于二分割中。

CART的著作中集中讨论了Gini规则，它与著名的信息增益和信息熵非常类似。对于一个二元(0/1)目标，节点t的“Gini杂质度”表示如下

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/10.1.png)

这里的p(t)是在节点中类1的（可能加权）相对频率，通过分割父节点P为左孩子结点L和右孩子节点R产生的提升(增益)如下表示

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/10.2.png)

这里q是走向左边的实例的(可能加权)部分。CART作者之所以喜欢Gini标准胜过信息增益的原因在于Gini可以很容易的被扩展以包括对称代价(见下面)，并且它比信息增益更加计算的更快。(CART更新的版本已经将信息增益作为一个可选的分割规则。)他们引入了改进的二分规则，它基于对目标属性分布在两个孩子节点上的直接比较：

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/10.3.png)

这里的k指示目标类，P_L()和P_R()是目标分别在左孩子和右孩子节点的概率分布，功率项u在生成大小不等子节点的分割上嵌入用户可控的处罚。(这个分离器是Messenger和Mandell的改进版本。)他们也引入了很多二元分割的标准来按照顺序处理对象的类；这种排序的二元分割试图保证左边表示的对象类排序低于右边代表的对象类。在我们的经验之中，二分标准在多类目标和本质上很难预测的二元对象(比如噪声)中表现一样优秀。对于回归（连续目标），CART提供了最小二乘法（LS）和最小绝对偏差（LAD）标准的选择，作为衡量拆分改进的基础。在下面单独讨论其他三种代价敏感学习的分割规则和概率树。

## 先验概率和类平衡

在默认的分类模式中，CART总是计算任何与根节点中类频率有关的节点的类频率。这等同于自动为数据重新加权从而平衡类，并且保证了选择的树最佳得最小化了平衡类误差。这样的权重调整在所有的概率和提升下都易于计算并且不需要用户的介入；记录的样本在每个节点中计数，这样就反映出未加权的数据。对于二元(0/1)目标，任何节点被归为类1时当且仅当

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/10.4.png)

在著作中，这样的默认模型被称为“先验相等(priors equal)”。它允许CART的用户在不需要特殊的关于类再平衡的测度或者引入人工构造权重的情况下更容易使用不平衡的数据。为了有效的处理不平衡的数据，使用CART的默认模式就足够了。隐式重新加权可以通过选择“先验数据”选项来进行关闭，建模者也可以选择指定任意一组先验来反映代价或者训练数据与未来数据目标类分布之间的潜在差异。

## 处理丢失数据

在真实的世界中常常会发生数据丢失的情况，尤其是一些与商业相关的数据库，如何处理这些丢失的数据对于所有的建模者而言都是一个令人困扰的挑战。CART的主要贡献之一就是提供了完全自动和高度有效的处理丢失数据的机制。决策树在三种层面上需要丢失数据处理机制(a)分离器评估时(b)当通过节点移动训练数据时(c)当通过一个节点移动测试数据来获得最终类别的时候(参考另一篇论文，其中有关于这些点的详细讨论)。对于(a)来说,CART的第一个版本对分离器可用的子数据集严格评估每个分离器的性能。后来的版本提供了一系列减少分离提升测度的惩罚来作为丢失程度的函数。对于(b)和(c)，无论丢失的数据是否发生在训练数据中，CART的机制都会为树里的每个节点找到“代理”或是替代的分离器。这样将树应用到新的数据时，即使包含丢失的值，也可以使用替代的分离器。这与那些只能从包含缺失数据的训练数据中学到如何处理丢失数据的机制是相反的。Friedman建议将有丢失分离器属性的实例都移动到左孩子节点和右孩子节点，最后通过合并所有实例出现的节点来分配最后的类别。Quinlan在他的关于另外一种处理遗失数据方法的研究中使用了Friedman方法加权的变体。我们对CART替代性能处理丢失数据的有效性评价是良好的，然而Quinlan基于他做的测试认为CART替代对于丢失数据的处理效果的性能是不可知的。Freidman等人提到CART中50%的代码都是为了处理丢失数据；这样Quilan在试验中使用的CART版本不可能正确的重复完整的CART替代机制。

在CART中遗失数据处理机制是全自动的并且局部适用于每个节点。在树的每个节点中，所选择的分离器引发数据的二进制分割(例如X1<=c1和X1>c1)。一个替代的分离器是单属性Z，当替代是一个二元分离器(例如Z<=d和Z>d)的时候，它就能够预测这个划分。换句话说，每个分离器变成了使用单个分裂二叉树来预测的新目标。按照关联评分对代理进行排名，该关联评分衡量了代理优于默认规则（预测所有案例都转到较大子节点）的优势。当CART树遇到了一个丢失的值，实例将根据排序最高的代理来确定移动到左子树还是右子树。如果这个代理也没了，那就使用排序第二高的代理（以此类推）。如果所有的代理都没有，默认的规则会把它分配给更大的子节点(可能调整先验的节点大小)。平局将会因实例移动到左边而被打破。

## 属性重要性

属性的重要性基于在属性作为分离器出现时（由每个节点分组中的训练数据的分数加权），所有节点提升的总和。替代也要包含在重要性的计算中，这意味着即使在变量从未分离节点也有可能分配很大的重要性分数。这样使用变量重要性排名可以揭示变量隐藏的价值和属性间的非线性相关性。重要性分数可以选择限制在分离器中，并且只在分离器中比较，完整的重要性排名是一个有用的诊断方法。

## 动态特征构建

Friedman讨论了在每个节点中新特征的自动构造，并且对于二元目标建议添加一个特征：x*w，这里的x是原始的属性向量，w是两个类之间的比例均值向量差异(Fisher线性判别方向)。这就好像对节点中所有可用的属性进行逻辑回归然后使用估计分对数作为预测器一样。在CART的著作中，作者提到了包括特征选择的线性组合的自动构造；这项功能从CART第一代的软件就已经有了。BFOS也提出了一个在每个节点中构造分离器的布尔组合的方法，这个功能在后来发布的软件中也没有出现。

## 成本敏感的学习

对于统计决策理论，成本是一个核心，但是在Domingos之前代价敏感的学习都没有获得多大的注意。在那之后，几个专门讨论这个话题的会议，并且有大量的研究论文在随后的科学文献中出现。因此，有必要注意到在CART的著作中引入了这两种代价敏感的学习策略，和描述CART在错误分类代价方面计算的整个数学机制。C(i,j)表示将一个为i的类别误分为类j的代价，同时除非有特别说明否则假设它等于1；对于所有的i，C(i，i)=0。完整的代价集表示为矩阵C，矩阵中对于每个目标类包含一行和一列。每个分类树都可以通过将所有误分类的代价相加为终端节点分配计算总代价。在代价敏感学习中的问题在于要考虑在树的成长和修剪过程的代价。

最简单直观的处理代价的方法是使用权重：Ting最近重新发现了一个方法，对于一个给定类别的所有实例都分配一个普通的权重，对于误分类代价高昂的实例增加权值。正如在CART中使用的那样，加权的过程是完全透明的，这样使得所有节点数以其原始未加权形式报告。对于多类问题，BFOS建议将误分类代价矩阵中的每行相加来获得相对的类权重，这个可以大致反应代价。这种技术虽然忽略了矩阵里的细节，但是由于其简单性，得到了广泛的使用。对于Gini分割规则，CART的作者表示只要整个代价矩阵可以被对称化，它就能够被嵌入到分割规则之中。“symGini”分割规则产生的树对代价C(i,j)和C(i,k)的差异是敏感的，当对称代价矩阵变成决策者问题可接受的形式时，“symGini”就会非常实用。相反，实例加权方法将代价分配给类i对象的所有误分类。BFOS表示在修剪树时使用完整的代价矩阵对于成功的代价敏感学习是至关重要的。

## 停止规则，修剪，树序列和树选择

决策树最早的工作不允许进行修剪。相反，树一直增长到达到某个停止条件，然后这个树就作为最后的结果。在CART的著作中，作者提到没有任何停止树增长的条件可以保证树不错过任何重要的数据结构(例如，考虑二维的异或问题)。因此，他们选择让树无限制的生长。最后产生的过大的树提供了可以提取出最后模型的原材料。

修剪的机制严格基于训练数据，并且以一个如下定义的代价复杂测度作为开始：R_a(T)=R(T)+a|T|,这里的R(T)是树的训练样本代价，|T|是树的终端节点的个数，a是每个节点的惩罚参数。如果a=0，最小的代价复杂度树就是最大可能的情况。如果a逐渐增加，最小的代价复杂度树会变小，因为至少在树底部减少R(T)的分割会被砍掉。这个参数是一个从0到一个足够减去所有分支的值之间逐渐增加的值。BFOS证明了任何以这种方式提取的大小为Q的树，会有一个代价R(Q)，它是有Q个终端节点的所有树的类里最小的。

最佳树的定义是树在修剪序列中达到测试数据的最低代价。因为测试误分类代价测度会收到抽样误差的影响，要修剪哪一个序列是最优的通常是一个不确定的问题。BFOS建议选择“1 SE”树。“1 SE”树指的是估计代价在最小代价(或“0 SE”)树的1标准误差之内的最小树。

## 概率树

最近在一些富有见解的文章中阐述了概率树的性质并且在寻求一些能够提高它性能的方法(见Provost 和Domingos 2002)。CART是第一个详细介绍概率树的著作，并且在CART的软件中也提供了对类概率树专用的分割规则。分类树和概率树之间最关键的不同在于后者希望将产生终端节点孩子的分割指派到相同的类下，但是前者不是如此(考虑到分类准确性，这样的分割无法实现任何东西)。同样，概率树的修剪方式也与分类树不同，它们两个最后的最优树的结构也会有些不同(虽然差异通常不大)。概率树基本的缺点在于基于终端节点的训练数据的概率估计通常会有偏差(例如在二元目标中偏向0或者1)，并且误差随着节点层数的增加而增加。在最近的机器学习文献中，建议使用LaPlace调整法来减小这个偏差(Provost 和Domingos 2002)。CART的著作中也提供了一种更为复杂的方法来调整终端节点的估计，但是它在文献中几乎没有被讨论。“Breiman调整”通过下面的公式来调整终端节点的估计误分类率r*(t)

![](https://github.com/ankang1993/data-mining-algorithms/blob/master/figure/10.5.png)

这里的r(t)是节点的训练样本估计，q(t)是节点训练样本的分数，S和e是用来求解对于给定树的训练和测试错误率差异的函数的参数。不同于Laplace方法，Breiman调整不依赖于节点原始的预测概率，并且如果测试数据表明树没有过拟合的话，调整可以非常小。Bloch等人提到Breiman调整在一系列经验实验中有非常好的表现。

## 理论基础

决策树最初的工作是与理论无关的。树被提出作为似乎有用的方法，和关于它们属性的讨论是基于在少数经验实例上观察树的性能得到的。然而这种方法在机器学习领域仍然很流行，最近在学科里的趋势更是希望有坚实的理论基础。CART著作中对理论高明的处理，并对几个关键结果提供了重要的技术观点和证明。举例而言，作者从期望误分类率推演到最大(最大可能)树，并且表明它的上限是贝叶斯率的两倍。作者也讨论了树中的偏方差权衡，并且说明了偏差是怎么被属性的数量影响的。基于CART前期大量的工作，共同作者Richard Olshen和Charles Stone在专著的最后三章将CART与最近邻居的理论工作联系起来，并且显示随着样本大小趋于无穷大，下面这些将会保持：(1)回归函数的估计收敛于真实的函数，(2)终端节点的风险收敛于相应的贝叶斯规则的风险。换句话说，非正式的说在有足够大样本的情况下，CART树会收敛到将目标与预测器相关联的真实函数，并达到最小的代价可能(贝叶斯率)。事实上来说，这样的结果或许只能出现在样本大小远远大于今天通常使用的大小。
